{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d400cb80-be73-40fd-8eff-c53f2ba97f6f",
   "metadata": {},
   "source": [
    "# What's In A Name? Investigating Bias And Identity In Text-To-Image Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This work will investigate the relationships between names and biases in text to image models. some narrower topics of interest include:\n",
    "\n",
    "* constructing measures for characterizing the degree to which a model has learned the relationship between a name and specific entity\n",
    "* measuring biases associated with names that are not tightly bound to identities\n",
    "* characterizing how biases of these kinds can be composed into new identities\n",
    "* characterizing the \"stickiness\" of specific attribute biases to identities/names\n",
    "* investigating the potential for leveraging these measure to monitor training progress for personalization finetuning and general purpose pre-training\n",
    "\n",
    "\n",
    "* biases in names\n",
    "* known identities (\"strong entities\")\n",
    "* overfitted identities (memorized images)\n",
    "  * mona lisa?\n",
    "\n",
    "* Just investigating biases, not making normative statements\n",
    "\n",
    "* associated hypotheses\n",
    "* out of scope hypotheses (future work)\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "1. The more weakly a name carries learned biases, the more closely the distribution of attributes observed when prompting that name approximates the model's global prior for the biases in those attributes. Consequence: low bias names can be used to probe model bias (assuming such names can be identified)\n",
    "\n",
    "2. \"pseudo-identities\" can be composed for prompt-engineering in text space, similar to how celeb-basis composes identities in token space.\n",
    "\n",
    "3. \"Identity strength\" can be quantified wrt...\n",
    "  * rigidity of distribution of attributes generated by a given(fixed) prompt\n",
    "  * flexibility to prompt identity into new scenarios (e.g. \"sks shaking hands with albert einstein\")\n",
    "\n",
    "4. identity evolves following a particular pattern, and monitoring this pattern can be used to evaluate model fit/grokking (e.g. for LoRA)\n",
    "  \n",
    "5. identity/bias-preserving representations can be crystallized from pre-trained representations, and leveraging these should permit measuring identity strength/rigidity more efficiently than with \"un-crystallized\" pre-trained representations.\n",
    "\n",
    "## Methodology [wip]\n",
    "\n",
    "* choosing names\n",
    "* generating images\n",
    "\n",
    "## Experiments\n",
    "\n",
    "* calibrating minimum # images\n",
    "* investigating which embedding representation to use\n",
    "* experimenting with summary statistics\n",
    "\n",
    "## Insights\n",
    "\n",
    "* presumptive phases of bias formation\n",
    "* correlation between age bias and popularity of names by birth year\n",
    "* strength of ethnic bias reflects undersampled data class\n",
    "  * strong biases hint at low data cardinality for the given class\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "\n",
    "## Future Work\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "* CLIP\n",
    "* Stable Diffusion\n",
    "* Stable Biases\n",
    "* Celeb Basis\n",
    "\n",
    "## Appendix\n",
    "\n",
    "* Calibration plots\n",
    "* DINO-vits stuff (assuming DINO-vitg shows in main report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
